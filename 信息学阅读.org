* 信息论阅读								:信息论:
<h2>
** 1. 阅读 http://www.cs.cmu.edu/~dst/Tutorials/Info-Theory/ </h2>
*** 1. 信息量是使用 bit 来表示的。
*** 2. 信息学的理论基础就是编码，而不是研究信息学的哲学本质。
*** 3. 用更少的 bit 表示更多的信息。
*** 4. 当事件的差异化越大时，可以用更少的 bit 来表示信息。
:NOTE:
极端的情况就是，如果扔1 million 次硬币，一直只有一面，那么就可以用 1 bit 来表示。
但是如果 million 次中，非常平均，那么就要用1 million bit 来表示。
但是，如果1000 次中，head 和 tail 的数目不一致，例如 head 的概率是 0.001，tail 的概率是 0.999.
那么就可以用 head 来表示。 1 million 次中 ，head 的次数平均是 1000 次，我们可以标记这些 head position，每一个
使用的 bit 是 log(1000000) = 20,全部的bit 是 1000 * 20 = 20000, 要远小于 1 miliion 。

当然，我们也可以使用 position 之间的距离来表示，平均来说，距离是 1000，那么就是 10 bit. 一共1000 个点，bit 总数
就是10000，要远小于 20000.
:END:

*** 5. 编码的极限(optimal encoding)
The information content of a sequence is defined as the number of bits required to transmit that sequence using
an optimal encoding. We are always free to use a less efficient coding, which will require more bits, but that
does not increase the amount of information transmitted.
序列的信息量，就是需要用来传输这个序列的最优编码位数。有时候，我们可能无法使用最优的编码，但是这个不影响传输的信息量，因为信息量的衡量是基于最优编码。
*** 6. 定长编码和变长编码
实际上就是在算法中学习的霍夫曼编码，出现概率小的事件用长编码，出现概率大的用长编码。实际上就是求最小值：
sum(编码长度*出现的概率)。

例子：
事件出现的概率：
A are 1/2, the chances of throwing a B are 1/4, C is 1/8, D is 1/16, E is 1/32, F is 1/64, and G and H are
each 1/128.

每个事件编码长度：A, we will transmit a single 0. If we throw a B we will transmit a 1 followed by a 0, which we'll write 10. If we throw a C the code will be 11 followed by 0, or 110. Similarly we'll use 1110 for D, 11110 for E, 111110 for F, 1111110 for G, and 1111111 for H.

总体事件的编码长度：
1/2 + 2/4 + 3/8 + 4/16 + 5/32 + 6/64 + 7/128 + 7/128 = 1.984
普通的编码方式: 8个事件，3 bit。 3 > 1.984
*** 7. 如何计算信息量
之前的例子，我们还设计了编码方式，然后求解最优，也就是说，我们需要知道最优编码，然后根据概率计算最优的bit。
那么，有没有可以不求解最优编码，就知道最优的bit。这个就是信息量的计算。

对于平均事件，事件的总数决定了 bit 数目，就是 log(num)。这个是理论上的最优解，例如：log(6) = 2.58.但是实际上，我们需要3bit，平均来说可以使用 2.58 bit。 理论上，我们可以有更多的方法来减少bit，例如把 3个事件当成一个连续的事件，那么只要8bit就可以了，单独看待需要 9 bit：
Instead of treating throws individually, consider them three at a time. The number of
possible three-throw sequences is 6^3= 216. Using 8 bits we can encode a number between 0 and 255, so a
three-throw sequence can be encoded in 8 bits with a little to spare; this is better than the 9 bits we'd need
if we encoded each of the three throws seperately.

这里使用了一个技巧，就是之前，计算 bit 数目，我们使用了 log，参数是 num，对于平均事件，每个事件的概率是 1/num.
那么需要的bit 数目实际上是 log(num) = log(1/P) = log(1) - log(p) = - log(p).

这个方法同样适用于非平均事件，因为 事件的 P 越大，那么这个事件的编码长度必然越小。对于 概率为 P 的事件，它的最长
编码是 - log(P). 这个通过观察之前的例子可以发现，概率 1/128 的事件，编码长度是7.

The expected number of bits required to encode one value is the weighted average of the
number of bits required to encode each possible value,where the weight is the probability of that value:
Underscript[∑,x] P (x) * -log P (x)

在计算一组事件的时候，在没有任何信息的情况下，我们只能假设事件之间独立，且每个事件都是平均分布。因为我们
发现，事件之间存在联系，就会减少 bit 数目。
因此，对一组事件描述需要的 bit 总数(使用概率，实际上也是一种归一化)。任何一个平均事件，使用的最优编码，不可能
超出 - log(p). 1/128的概率，实际上就是这个事件需要和其余的 127 种情况区分，才能有效表示，因此最优的编码长度是7.

实际上，这个就是知道事件发生概率的情况下的基础信息量的计算方法。P(A)

With a coin there are only two possible values. What information theory says we can do is consider each value
separately.  If a particular value occurs with probability P, we assume that it is drawn from a uniformly
distributed set of values when calculating its information content. The size of this set would be 1/P elements.
Thus, the number of bits required to encode one value from this hypothetical set is -log P. Since the actual
distribution we're trying to encode is not uniform, we take the weighted average of the estimated information
content of each value (heads or tails, in the case of a coin), weighted by the probability P of that value
occuring. Information theory tells us that an optimal encoding can do no better than this. Thus, with the
heavily biased coin we have the following:

    P(heads) = 1/1000,  so heads takes -log(1/1000) = 9.96578 bits to encode

    P(tails) = 999/1000,  so tails takes -log(999/1000) = 0.00144 bits to encode

    Avg.bits required  =  Underscript[∑, x] -P(x) log P(x)
        = (1/1000) × 9.96578 + (999/1000) × 0.00144  =  0.01141 bits per coin toss 

也就是说，在 TIPS 中，描述 pattern 出现在某个 bin 中需要的 bit 数目。理论上，5 bin 需要的是 2.32,也就是3 bit。
但是，分布更加有特征的pattern，需要的 bit 数目更小。

这个就是 information content。

DTIPS 计算的就是，当知道了 action 在某个bin 发生，且知道是否属于某个 pattern，那么这个action 属于某个组的事件
的描述bit 是多少。需要的bit 越少，说明事件更为清晰。
<h2> 
** 2. 阅读 http://moultano.wordpress.com/article/a-short-simple-introduction-to-3kbzhsxyg4467-7/ </h2>
*** 1. rare == information.稀少即为信息，这个是一个很通俗易懂的解释。
The rarer something is, the more you’ve learned if you discover that it happened.
*** 2. 信息熵没有负值.这个和之前对熵的理解不同。 
